
1. LLM 的评分准确吗? -- 基本OK, 没有对齐的情况之后重新测试一下 ; 两次测试的一致性如何? 
2. badcase: 在哪几个场景下表现较差? 
3. json 解析出错 -- 发现eval的兼容性更好






## 240905 进度讨论
1. Workflow构造: 用户query + 网页检索 -> 自动化 生成 (based on the first page) + 改写 (w/ following pages) PDL
2. 使用画布的三种角色
    Demands: 画布制作方 
    LLM: 模拟人 (客服), 来去执行一个流程
    User: 用户, 交互的对象是自动化的agent
3. 画布制作方式: 1) 人写; 2) 有数据来源 (wikihow, web/doc); 3) 从需求出发直接造
    如何修改? #TODO: [in the future]
        1) 更自由的数据 (but just prompt)
        2) 交互式的画布构造: 用户给了需求 -> 给个草稿 -> 发现问题并对用户提问 -> 修改 (重复流程) [甲方乙方]
4. PDL 生成: 给定一组已有的 API (tools), 实现某一个需求 -> 这种生成方案是可以固化为 Workflow 的 (new tool). 


## 0902 PDL 核心点的思考

从我的角度, PDL 核心的出发点是: 在业务 (具体任务) 场景下, 不同方案在面临 **Reliability v.s. Flexibility** 之间的平衡, 我们提出了一种新的语法来对于任务流程进行描述, 并在此基础上开发了一套框架进行执行.

1. 在PDL的语法层面, 包括了核心的 procedure description + APIs + task defination 等组件, 相较于画布形式的或者CoRE语法等更加自由. -- 不过需要结合实验结果来验证这种形式带来的和其他语法之间的本质差异
2. 从执行框架的角度, 我们的PDL的基础上引入了一套辅助执行逻辑, e.g. 
    1. 节点依赖以控制流程跳转;  API访问限制以避免陷入死循环; 用户自定义指令以增加灵活性 -- 引入了一些外部控制功能
    2. 判断的时候引入 -- 例如对于数字大小判断, 日期等更精确
3. 从数据构造的角度, NL->PDL 的自动转化; 还有我们除了在PDL之外, 数据还包括了 user profile -- 这个是在此前数据集里面没有的


## 0827 评测方案小结

近期评测方面的结论: 

1. 评测数据的质量很重要, 质量的问题会导致用户侧不知道作什么询问, 导致流程无法进行. "质量" 主要包括 (1) PDL本身流程和合理性和完备性; (2) API 底层数据和回传错误的信息量. 
2. 虽然现在实现了 PDL 和线上三节点方案的评测框架, 但整体上无法对齐, 主要是因为API设置不同. 在已有的5张画布上的实验发现三节点方案的结果基本不可用. 
3. 指标设计: (1) 两个维度, 从单轮次和整体对话进行粗细粒度的评估. (2) 错误类别划分, 包括 流程, API, SLOT, ANSWER 类, 对应PDL/三节点方案的不同流程. 

后续计划: 

1. 构造新的测试数据, 主要目标是优化API的实现, 同时控制PDL的流程整体上是合理的. 采用真实的API进行测试. 预计本周内完成 5~10 张画布用于测试. 
2. 在此基础上完成两种方案的比较, 在基本设置上实现对齐; 仍然用LLM模拟用户进行交互, 比较的是 "真实场景" 下的可用性和用户满意度. 本周内给出基于LLM的评测结果比较. 




